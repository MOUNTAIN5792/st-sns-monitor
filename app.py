import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="è¶…çµ¶ãƒã‚ºç›£è¦–ãƒœãƒ¼ãƒ‰", layout="wide", page_icon="ğŸš€")

st.title("ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¶…çµ¶ãƒã‚ºï¼ˆ1000ã„ã„ã­ä»¥ä¸Šï¼‰ç›£è¦–")
st.caption("SNSä¸Šã§ä»Šã¾ã•ã«1000ã„ã„ã­ã‚’è¶…ãˆã¦ã„ã‚‹æ³¨ç›®ã®æŠ•ç¨¿ã‚’è¡¨ç¤ºã—ã¾ã™")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
st.sidebar.header("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿")
default_query = "ã® min_faves:1000"
search_keyword = st.sidebar.text_input("æ¤œç´¢ã‚³ãƒãƒ³ãƒ‰", default_query)
update_interval = st.sidebar.slider("è‡ªå‹•æ›´æ–°ã®é–“éš” (ç§’)", 30, 300, 60)

def get_trends(keyword):
    url = f"https://search.yahoo.co.jp/realtime/search?p={keyword}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        items = []
        posts = soup.find_all(["p", "span"], class_=lambda x: x and ("Tweet_body" in x or "Content" in x))
        
        for post in posts[:20]:
            text = post.get_text()
            if text and len(text) > 10:
                items.append({
                    "å–å¾—æ™‚åˆ»": datetime.now().strftime("%H:%M"),
                    "ãƒã‚ºæŠ•ç¨¿å†…å®¹": text
                })
        return pd.DataFrame(items)
    except:
        return pd.DataFrame()

# ãƒ¡ã‚¤ãƒ³è¡¨ç¤ºã‚¨ãƒªã‚¢ï¼ˆã“ã“ã‹ã‚‰ä¸‹ã®æ®µè½ãŒé‡è¦ã§ã™ï¼‰
placeholder = st.empty()

while True:
    with placeholder.container():
        df = get_trends(search_keyword)
        if not df.empty:
            st.write(f"æœ€çµ‚æ›´æ–°: {datetime.now().strftime('%H:%M:%S')}")
            st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            st.warning(f"ã€Œ{search_keyword}ã€ã«ä¸€è‡´ã™ã‚‹æŠ•ç¨¿ãŒã¾ã è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    time.sleep(update_interval)
