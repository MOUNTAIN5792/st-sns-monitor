import streamlit as st
import streamlit.components.v1 as components
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import urllib.parse

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="ã‚¨ã‚¹ãƒ†ãƒ¼å…¬å¼æŠ•ç¨¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", layout="centered", page_icon="ğŸ¤")

# Xï¼ˆTwitterï¼‰ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆç”¨JavaScriptã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
def render_tweet(tweet_url):
    embed_code = f"""
    <div style="display: flex; justify-content: center;">
        <blockquote class="twitter-tweet" data-conversation="none" data-theme="light">
            <a href="{tweet_url}"></a>
        </blockquote>
    </div>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    """
    components.html(embed_code, height=600, scrolling=True)

st.title("ğŸ¤ @st_product_info æˆæœæŠ•ç¨¿æŠ½å‡º")
st.caption("ã‚¨ã‚¹ãƒ†ãƒ¼å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®éå»ã®ãƒã‚ºæŠ•ç¨¿ã‚’å…¬å¼åŸ‹ã‚è¾¼ã¿ã§å†ç¾ã—ã¾ã™")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæ¤œç´¢æ¡ä»¶ ---
st.sidebar.header("ğŸ” æŠ½å‡ºæ¡ä»¶")
MY_X_ID = "st_product_info"

# æœŸé–“æŒ‡å®šï¼ˆåˆæœŸå€¤ã¯ç›´è¿‘180æ—¥ï¼‰
start_date = st.sidebar.date_input("é–‹å§‹æ—¥", datetime.now() - timedelta(days=180))
end_date = st.sidebar.date_input("çµ‚äº†æ—¥", datetime.now())

# ã„ã„ã­æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
min_faves = st.sidebar.slider("æœ€ä½ã„ã„ã­æ•°", 0, 5000, 100)

# å®Ÿè¡Œãƒœã‚¿ãƒ³
if st.sidebar.button("å®Ÿç¸¾ã‚’æŠ½å‡ºã™ã‚‹"):
    # Yahoo!ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢ç”¨ã®ã‚¯ã‚¨ãƒªä½œæˆ
    # æ›¸å¼: from:ID min_faves:æ•°å­— since:YYYY-MM-DD until:YYYY-MM-DD
    query = f"from:{MY_X_ID} min_faves:{min_faves} since:{start_date} until:{end_date}"
    encoded_query = urllib.parse.quote(query)
    url = f"https://search.yahoo.co.jp/realtime/search?p={encoded_query}"
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
    
    with st.spinner('ã‚¨ã‚¹ãƒ†ãƒ¼ã®éå»æŠ•ç¨¿ã‚’æ¢ç´¢ä¸­...'):
        try:
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.text, "html.parser")
            
            # æŠ•ç¨¿URLï¼ˆtwitter.com/x.com ã® status ã‚’å«ã‚€ã‚‚ã®ï¼‰ã‚’æ­£è¦è¡¨ç¾ã§æ¢ã™
            links = soup.find_all("a", href=re.compile(r'(twitter\.com|x\.com)/.+/status/\d+'))
            
            # URLã‚’é‡è¤‡ãªããƒªã‚¹ãƒˆåŒ–ï¼ˆã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ï¼‰
            tweet_urls = []
            for l in links:
                base_url = l['href'].split('?')[0]
                if base_url not in tweet_urls:
                    tweet_urls.append(base_url)
            
            if tweet_urls:
                st.success(f"{len(tweet_urls[:10])}ä»¶ã®ãƒã‚ºæŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆä¸Šä½10ä»¶ã‚’è¡¨ç¤ºï¼‰")
                for t_url in tweet_urls[:10]:
                    render_tweet(t_url)
            else:
                st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–‹å§‹æ—¥ã‚’å¤ãã™ã‚‹ã‹ã€ã„ã„ã­æ•°ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚")
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œå®Ÿç¸¾ã‚’æŠ½å‡ºã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€ã‚¨ã‚¹ãƒ†ãƒ¼å…¬å¼ã®éå»æŠ•ç¨¿ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
