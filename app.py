import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Poisdex æŠ•ç¨¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", layout="wide", page_icon="ğŸ“Š")

# Xï¼ˆTwitterï¼‰é¢¨ã®ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ãƒ»ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ã‚¶ã‚¤ãƒ³
st.markdown("""
    <style>
    .stApp { background-color: #f7f9f9; }
    .tweet-box {
        background-color: white;
        border: 1px solid #e1e8ed;
        border-radius: 16px;
        padding: 20px;
        margin-bottom: 20px;
        max-width: 600px;
        margin-left: auto;
        margin-right: auto;
        transition: 0.3s;
    }
    .tweet-box:hover { background-color: #f8f8f8; border-color: #ccc; }
    .user-info { font-weight: bold; color: #0f1419; margin-bottom: 5px; }
    .user-id { color: #536471; font-weight: normal; font-size: 0.9em; }
    .tweet-text { font-size: 1.1em; color: #0f1419; line-height: 1.5; white-space: pre-wrap; }
    .tweet-footer { margin-top: 12px; color: #536471; font-size: 0.85em; border-top: 1px solid #eff3f4; padding-top: 10px; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ“Š @poisdex å®Ÿç¸¾æŠ•ç¨¿ã‚®ãƒ£ãƒ©ãƒªãƒ¼")
st.caption("æŒ‡å®šã—ãŸæœŸé–“ãƒ»åå¿œæ•°ã«åŸºã¥ãã€éå»ã®æˆæœã‚’æŠ½å‡ºã—ã¾ã™")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š ---
st.sidebar.header("ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š")
MY_X_ID = "poisdex"

# æ—¥ä»˜ç¯„å›²ã®é¸æŠ
col_date1, col_date2 = st.sidebar.columns(2)
start_date = col_date1.date_input("é–‹å§‹æ—¥", datetime.now() - timedelta(days=180))
end_date = col_date2.date_input("çµ‚äº†æ—¥", datetime.now())

# ã„ã„ã­æ•°ã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
min_faves = st.sidebar.slider("æœ€ä½ã„ã„ã­æ•°", 0, 1000, 50)

# è¡¨ç¤ºä»¶æ•°
limit = st.sidebar.selectbox("è¡¨ç¤ºä»¶æ•°", [5, 10, 20, 50], index=1)

# --- ãƒ‡ãƒ¼ã‚¿å–å¾— ---
def fetch_my_best_posts():
    # æ¤œç´¢ã‚³ãƒãƒ³ãƒ‰ã‚’çµ„ã¿ç«‹ã¦
    query = f"from:{MY_X_ID} min_faves:{min_faves} since:{start_date} until:{end_date}"
    url = f"https://search.yahoo.co.jp/realtime/search?p={query}"
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        
        # Yahoo!ã®æ§‹é€ ã‹ã‚‰æŠ•ç¨¿æœ¬æ–‡ã‚’å–å¾—
        posts = soup.find_all(["p", "span"], class_=lambda x: x and ("Tweet_body" in x or "Content" in x))
        
        if not posts:
            st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        st.subheader(f"âœ¨ æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æŠ•ç¨¿ï¼ˆä¸Šä½ {min(len(posts), limit)} ä»¶ï¼‰")
        
        for post in posts[:limit]:
            text = post.get_text()
            if len(text) > 5:
                # æŠ•ç¨¿ã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
                st.markdown(f"""
                <div class="tweet-box">
                    <div class="user-info">poisdex <span class="user-id">@poisdex</span></div>
                    <div class="tweet-text">{text}</div>
                    <div class="tweet-footer">ğŸ“Š ã“ã®æœŸé–“ã®æˆæœæŠ•ç¨¿</div>
                </div>
                """, unsafe_allow_html=True)
                
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

# --- å®Ÿè¡Œ ---
if st.sidebar.button("ã“ã®æ¡ä»¶ã§æŠ½å‡ºå®Ÿè¡Œ"):
    with st.spinner('å–å¾—ä¸­...'):
        fetch_my_best_posts()
else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒæŠ½å‡ºå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨è¡¨ç¤ºãŒå§‹ã¾ã‚Šã¾ã™ã€‚")
